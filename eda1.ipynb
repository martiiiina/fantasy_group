{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8226207",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad34a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_csv_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mie import split_categorical_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd56d8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "thresh_quant = 0.9\n",
    "thresh_corr = 0.9\n",
    "thresh_nan = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ef819",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/dataset/dataset'\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path, sub_sample=False)\n",
    "\n",
    "print(\"Number of samples of train: \", x_train.shape[0])\n",
    "print(\"Number of features: \", x_train.shape[1])\n",
    "print(\"Number of samples of test: \", x_test.shape[0])\n",
    "print(\"Data type x_train:\", x_train.dtype) \n",
    "print(\"Data type y_train:\", y_train.dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29e04f",
   "metadata": {},
   "source": [
    "Types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed threshold\n",
    "threshold = 20\n",
    "categorical_idx, continuous_idx, unique_counts = split_categorical_continuous(x_train, threshold=20)\n",
    "\n",
    "print(f\"Total categorical features (<= {threshold} unique): {len(categorical_idx)}\")\n",
    "print(f\"Total continuous features    (>  {threshold} unique): {len(continuous_idx)}\")\n",
    "\n",
    "unique_counts = np.array(unique_counts)\n",
    "unique_vals, counts = np.unique(unique_counts, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(unique_vals, counts, color='lightblue', width=1.0)\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')\n",
    "plt.xlabel('Number of unique values')\n",
    "plt.xlim(right=200)\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel('Number of features')\n",
    "plt.title('Distribution of unique values per feature')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeec30",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_categorical(X,  categorical_idx, threshold_q):\n",
    "    \"\"\"\n",
    "    Replace anomalous or extreme values with np.nan\n",
    "    threshold = limit quantile (e.g. 0.99 = over the 99° percentile)\n",
    "    \"\"\"\n",
    "    X_clean = X.copy().astype(float)\n",
    "    for i in range(X.shape[1]):\n",
    "        if i not in categorical_idx: \n",
    "             continue\n",
    "        col = X[:, i]\n",
    "        col_valid = col[~np.isnan(col)]\n",
    "        if len(col_valid) == 0:\n",
    "            continue\n",
    "        max_val = np.percentile(col_valid, threshold_q * 100)\n",
    "        X_clean[col > max_val, i] = np.nan\n",
    "    return X_clean\n",
    "\n",
    "thresh_quant=0.9\n",
    "\n",
    "x_train_nan = remove_outliers_categorical(x_train,categorical_idx, thresh_quant)\n",
    "x_test_nan = remove_outliers_categorical(x_test,categorical_idx, thresh_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de517841",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_ratio = np.mean(np.isnan(x_train_nan), axis=0)\n",
    "print(f\"Number of features with more than 40% of NaN: {np.sum(nan_ratio>0.4)}\")\n",
    "\n",
    "# 1 Drop feature if at least 40% are NaN values\n",
    "#1st change: 30% instead of 40%\n",
    "valid_cols = np.where(nan_ratio < 0.4)[0]\n",
    "x_train_clean = x_train_nan[:, valid_cols]\n",
    "x_test_clean = x_test_nan[:, valid_cols]\n",
    "print(f\"Shape after dropping cols >40% NaN: {x_train_clean.shape}\")\n",
    "\n",
    "# 2 Identify categorical vs continuous\n",
    "categorical_idx, continuous_idx, _ = split_categorical_continuous(x_train_clean, threshold=20)\n",
    "print(f\"Categorical features: {len(categorical_idx)}\")\n",
    "print(f\"Continuous features: {len(continuous_idx)}\")\n",
    "\n",
    "# 4. Impute NaN differently\n",
    "# 4a. Continuous → replace NaN with mean\n",
    "for i in continuous_idx:\n",
    "    median = np.nanmedian(x_train_clean[:, i])\n",
    "    x_train_clean[np.isnan(x_train_clean[:, i]), i] = median\n",
    "    x_test_clean[np.isnan(x_test_clean[:, i]), i] = median\n",
    "\n",
    "# 4b. Categorical → replace NaN with mode\n",
    "#def nanmode(col):\n",
    "    #vals, counts = np.unique(col[~np.isnan(col)], return_counts=True)\n",
    "    #return vals[np.argmax(counts)] if len(vals) > 0 else np.nan\n",
    "\n",
    "# 4b. Categorical → replace NaN with -1\n",
    "for i in categorical_idx:\n",
    "    #mode_val = nanmode(x_train_clean[:, i])\n",
    "    x_train_clean[np.isnan(x_train_clean[:, i]), i] = - 1\n",
    "    x_test_clean[np.isnan(x_test_clean[:, i]), i] = - 1\n",
    "\n",
    "print(\"Missing values imputed (median for continuous, -1 for categorical)\")\n",
    "print(\"Shape x_train_clean:\", x_train_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-correlation among features\n",
    "def corrcoef(X):\n",
    "    n_features = X.shape[1]\n",
    "    corr = np.empty((n_features, n_features))\n",
    "    for i in range(n_features):\n",
    "        for j in range(i, n_features):\n",
    "            corr_ij = np.corrcoef(X[:, i], X[:, j])[0, 1]       # np.corrcoef returns correlation matrix of two 1D-arrays, corr_ij is in position [0,1]            \n",
    "            corr[i, j] = corr_ij\n",
    "            corr[j, i] = corr_ij  \n",
    "    return corr\n",
    "\n",
    "corr_matrix = corrcoef(x_train_clean)\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation matrix before cleaning')\n",
    "plt.show()\n",
    "\n",
    "# Drop features if correlation > 0.9\n",
    "\n",
    "non_valid_col = set()  # Use set to avoid duplicates\n",
    "for i in range(x_train_clean.shape[1]):\n",
    "    for j in range(i + 1, x_train_clean.shape[1]):\n",
    "        if abs(corr_matrix[i, j]) > 0.9:\n",
    "            non_valid_col.add(j)  # Drop j, keep i\n",
    "\n",
    "x_train_decorr = np.delete(x_train_clean, list(non_valid_col), axis=1)     # Remove column\n",
    "x_test_decorr = np.delete(x_test_clean, list(non_valid_col), axis=1)\n",
    "print(f\"Shape of decorrelated X: {x_train_decorr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5785593",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc68d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_columns(X, categorical_cols):\n",
    "    \"\"\"\n",
    "    Applies one-hot encoding on categorical columns of x_train.\n",
    "    Returns the new array and a dictionary with used categories.\n",
    "    \"\"\"\n",
    "    X_encoded = []\n",
    "    category_map = {}  # to remind which categories has every column\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        if i in categorical_cols:\n",
    "            # unique values in the column\n",
    "            values = np.unique(X[:, i])\n",
    "            category_map[i] = values\n",
    "\n",
    "            # Creates a one-hot matrix for each value\n",
    "            one_hot = np.zeros((X.shape[0], len(values)))\n",
    "            for j, val in enumerate(values):\n",
    "                one_hot[:, j] = (X[:, i] == val).astype(float)\n",
    "            \n",
    "            X_encoded.append(one_hot)\n",
    "        else:\n",
    "            # leaves the numerical column unchanged\n",
    "            X_encoded.append(X[:, i].astype(float).reshape(-1, 1))\n",
    "    \n",
    "    # final concatenation of all columns\n",
    "    X_encoded = np.concatenate(X_encoded, axis=1)\n",
    "    return X_encoded, category_map\n",
    "\n",
    "def apply_one_hot_encoding(X, categorical_cols, category_map):\n",
    "    \"\"\"\n",
    "    Applies the encoding to the test set using categories already found on the training set.\n",
    "    If a category is not present in the training, it is ignored (0 column).\n",
    "    \"\"\"\n",
    "    X_encoded = []\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        if i in categorical_cols:\n",
    "            values = category_map[i]\n",
    "            one_hot = np.zeros((X.shape[0], len(values)))\n",
    "            for j, val in enumerate(values):\n",
    "                one_hot[:, j] = (X[:, i] == val).astype(float)\n",
    "            X_encoded.append(one_hot)\n",
    "        else:\n",
    "            X_encoded.append(X[:, i].astype(float).reshape(-1, 1))\n",
    "\n",
    "    X_encoded = np.concatenate(X_encoded, axis=1)\n",
    "    return X_encoded\n",
    "\n",
    "threshold = 20\n",
    "categorical_idx, continuous_idx, _ = split_categorical_continuous(x_train_decorr, threshold=20)\n",
    "\n",
    "X_train_encoded, category_map =  one_hot_encode_columns(x_train_decorr, categorical_idx)\n",
    "X_test_encoded = apply_one_hot_encoding(x_test_decorr, categorical_idx, category_map)\n",
    "print(X_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074011a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = np.linalg.matrix_rank(X_train_encoded)\n",
    "\n",
    "print(f\"Number of features: {X_train_encoded.shape[1]}\")\n",
    "print(f\"Rank of the matrix: {rank}\")\n",
    "\n",
    "if rank < X_train_encoded.shape[1]:\n",
    "    print(\"Some features are linearly dependent!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9d3a5",
   "metadata": {},
   "source": [
    "Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603339ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ricostruisci indici delle colonne continue dopo il one-hot encoding\n",
    "n_features_original = x_train_decorr.shape[1]\n",
    "continuous_mask = np.ones(n_features_original, dtype=bool)\n",
    "continuous_mask[categorical_idx] = False\n",
    "\n",
    "new_continuous_idx = []\n",
    "col_counter = 0\n",
    "\n",
    "for i in range(n_features_original):\n",
    "    if continuous_mask[i]:\n",
    "        # feature continua → una sola colonna\n",
    "        new_continuous_idx.append(col_counter)\n",
    "        col_counter += 1\n",
    "    else:\n",
    "        # feature categoriale → tante colonne quante categorie\n",
    "        n_values = len(category_map[i])\n",
    "        col_counter += n_values\n",
    "\n",
    "print(f\"Numero di colonne continue dopo one-hot: {len(new_continuous_idx)}\")\n",
    "\n",
    "col_mean = np.mean(X_train_encoded[:, new_continuous_idx], axis=0)\n",
    "col_std = np.std(X_train_encoded[:, new_continuous_idx], axis=0)\n",
    "\n",
    "x_train_norm = X_train_encoded.copy()\n",
    "x_train_norm[:, new_continuous_idx] = (X_train_encoded[:, new_continuous_idx] - col_mean) / col_std\n",
    "x_test_norm = X_test_encoded.copy()\n",
    "x_test_norm[:, new_continuous_idx] = (X_test_encoded[:, new_continuous_idx] - col_mean) / col_std\n",
    "\n",
    "col_mean_tr = np.mean(x_train_norm[:, new_continuous_idx], axis=0)\n",
    "col_std_tr = np.std(x_train_norm[:, new_continuous_idx], axis=0)\n",
    "\n",
    "print(f\"Before normalization, mean: {col_mean[0:3]}, std: {col_std[0:3]}\")\n",
    "print(f\"After normalization, mean: {col_mean_tr[0:3]}, std: {col_std_tr[0:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff377486",
   "metadata": {},
   "source": [
    "Classes distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=20, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribuzione delle etichette (y_train)\")\n",
    "plt.xlabel(\"Valore\")\n",
    "plt.ylabel(\"Frequenza\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db809d0c",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"processed/x_train.npy\", x_train_norm)\n",
    "np.save(\"processed/x_test.npy\", x_test_norm)\n",
    "np.save(\"processed/y_train.npy\", y_train)\n",
    "np.save(\"processed/train_ids.npy\", train_ids)\n",
    "np.save(\"processed/test_ids.npy\", test_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
